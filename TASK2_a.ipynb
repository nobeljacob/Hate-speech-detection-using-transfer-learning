{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "#from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import torch.nn as nn\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Reference---\n",
    "#https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('hindi_dataset.tsv',nrows=100)\n",
    "data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']= data['text'].str.lower()\n",
    "data['text'] = data['text'].str.replace('[!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]','')\n",
    "\n",
    "file1 = open(\"stopwords.txt\", encoding=\"utf8\") \n",
    "q=file1.read()\n",
    "split1=q.split()\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([item for item in x.split() if item not in split1]))\n",
    "\n",
    "data.loc[data['task_1'] == \"NOT\", 'task_1'] = 0\n",
    "data.loc[data['task_1'] == \"HOF\", 'task_1'] = 1\n",
    "\n",
    "X = data.iloc[:, 1].values  \n",
    "Y= data.iloc[:, 2].values  \n",
    "\n",
    "Y = np.asarray(Y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the embedding stored for task1\n",
    "model_path = \"task1.pth\"\n",
    "state_dict = torch.load(model_path)\n",
    "embedding_weights=state_dict['l2.weight']\n",
    "\n",
    "#retreiving the value of variable freq in this jupyter file so that words can be mapped to corresponding weights from task1\n",
    "with open('hindi_freq_dictionary.txt', 'rb') as handle:\n",
    "    data_dummy = handle.read()\n",
    "    \n",
    "freq = pickle.loads(data_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the number of unique words or vocabulary size\n",
    "entire_text = data['text'].values.tolist()\n",
    "entire_text= [word for line in entire_text for word in line.split()]\n",
    "\n",
    "\n",
    "unique_words_and_corresponding_freq = {} \n",
    "for item in entire_text: \n",
    "    if (item in unique_words_and_corresponding_freq): \n",
    "        unique_words_and_corresponding_freq[item] += 1\n",
    "    else: \n",
    "        unique_words_and_corresponding_freq[item] = 1\n",
    "        \n",
    "\n",
    "no_of_unique_words_vocabulary=len(unique_words_and_corresponding_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,random_state=1,test_size=0.20 )\n",
    "\n",
    "#tokenizer that is crated for the entire vocabulary \n",
    "tokenizer = Tokenizer(num_words=no_of_unique_words_vocabulary)\n",
    "tokenizer.fit_on_texts(X)\n",
    "word_index=tokenizer.word_index\n",
    "\n",
    "#fitting the tokenizor for each x_train and x_test data\n",
    "x_train_tokens=tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens=tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#to add padding based the longest sentence\n",
    "#code to find the lenght of the lpngest sentence\n",
    "\n",
    "longest_sentence=0\n",
    "\n",
    "for each_sentence in X:\n",
    "    word_list = each_sentence.split()\n",
    "    number_of_words = len(word_list)\n",
    "    if number_of_words > longest_sentence:\n",
    "        longest_sentence=number_of_words \n",
    "        \n",
    "\n",
    "X_train_with_padding=pad_sequences(x_train_tokens,maxlen=longest_sentence,padding='post')\n",
    "X_test_with_padding=pad_sequences(x_test_tokens,maxlen=longest_sentence,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the embedding matrix to use in task2\n",
    "embedding_matrix_to_use=np.zeros((len(word_index)+1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_index_in_the_embedding_from_task1(word):\n",
    "    return list(freq.keys()).index(word)\n",
    "    \n",
    "#test\n",
    "#print(return_index_in_the_embedding_from_task1('देश'))\n",
    "#\n",
    "\n",
    "#this function returns the curresponing index of the the word from task1.\n",
    "#the idea is to map the words in task2 to their curresponding weights in task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each word in task2 is mapped to its currensponding weight in task1 vocabulary \n",
    "for word,index in word_index.items():\n",
    "    \n",
    "    \n",
    "        corresponding_index_in_embedding= return_index_in_the_embedding_from_task1(word)\n",
    "        numpy_corresponding= embedding_weights[corresponding_index_in_embedding].data.numpy()\n",
    "        embedding_matrix_to_use[index]=numpy_corresponding\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "#print(\"Embedding Matrix Shape\",embedding_matrix_to_use.shape)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference link- https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "embedding_layer=Embedding(len(word_index)+1,300,weights=[embedding_matrix_to_use],input_length=longest_sentence,trainable=False)\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "model_keras=Sequential()\n",
    "\n",
    "model_keras.add(embedding_layer)\n",
    "model_keras.add(LSTM(units=32,dropout=0.1))\n",
    "model_keras.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model_keras.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_keras.fit(X_train_with_padding,y_train,epochs=10,validation_data=(X_test_with_padding,y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
