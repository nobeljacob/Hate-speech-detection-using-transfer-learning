{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import torch\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import argmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali_data = pd.read_csv(r'bengali_hatespeech.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_set_data= bengali_data.head(100)\n",
    "last_set_data=bengali_data.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[first_set_data, last_set_data]\n",
    "bengali_data = pd.concat(frames)\n",
    "bengali_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the data to lower text, remove punctuations if any and remove the emoji or emoticons\n",
    "\n",
    "bengali_data['sentence']= bengali_data['sentence'].str.lower()\n",
    "bengali_data['sentence'] = bengali_data['sentence'].str.replace('[!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]','')\n",
    "bengali_data['sentence'] = bengali_data['sentence'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "\n",
    "\n",
    "X = bengali_data.iloc[:, 0].values  \n",
    "Y= bengali_data.iloc[:, 1].values  \n",
    "\n",
    "\n",
    "\n",
    "Y = np.asarray(Y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the embedding stored for bengali\n",
    "\n",
    "model_path = \"bengali.pth\"\n",
    "state_dict = torch.load(model_path)\n",
    "embedding_weights=state_dict['l2.weight']\n",
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_text = bengali_data['sentence'].values.tolist()\n",
    "entire_text= [word for line in entire_text for word in line.split()]\n",
    "\n",
    "\n",
    "unique_words_and_corresponding_freq = {} \n",
    "for item in entire_text: \n",
    "    if (item in unique_words_and_corresponding_freq): \n",
    "        unique_words_and_corresponding_freq[item] += 1\n",
    "    else: \n",
    "        unique_words_and_corresponding_freq[item] = 1\n",
    "        \n",
    "\n",
    "no_of_unique_words_vocabulary=len(unique_words_and_corresponding_freq)\n",
    "no_of_unique_words_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,random_state=1,test_size=0.20 )\n",
    "\n",
    "#tokenizer that is crated for the entire vocabulary \n",
    "tokenizer = Tokenizer(num_words=no_of_unique_words_vocabulary)\n",
    "tokenizer.fit_on_texts(X)\n",
    "word_index=tokenizer.word_index\n",
    "\n",
    "#fitting the tokenizor for each x_train and x_test data\n",
    "x_train_tokens=tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens=tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to add padding based the longest sentence\n",
    "#code to find the lenght of the lpngest sentence\n",
    "\n",
    "longest_sentence=0\n",
    "\n",
    "for each_sentence in X:\n",
    "    word_list = each_sentence.split()\n",
    "    number_of_words = len(word_list)\n",
    "    if number_of_words > longest_sentence:\n",
    "        longest_sentence=number_of_words \n",
    "        \n",
    "\n",
    "X_train_with_padding=pad_sequences(x_train_tokens,maxlen=longest_sentence,padding='post')\n",
    "X_test_with_padding=pad_sequences(x_test_tokens,maxlen=longest_sentence,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the embedding matrix to use in task2\n",
    "embedding_matrix_to_use=np.zeros((len(word_index)+1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('bengali_freq_dictionary.txt', 'rb') as handle:\n",
    "    data = handle.read()\n",
    "    \n",
    "bengali_freq = pickle.loads(data)\n",
    "#len(bengali_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_index_in_the_embedding_from_task1(word):\n",
    "    return list(bengali_freq.keys()).index(word)\n",
    "   \n",
    "#return_index_in_the_embedding_from_task1('ফজলম')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retreive corresponding weights of words from weight matrix\n",
    "skipped_words=0\n",
    "for word,index in word_index.items():\n",
    "    \n",
    "    try:\n",
    "        corresponding_index_in_embedding= return_index_in_the_embedding_from_task1(word)\n",
    "        numpy_corresponding= embedding_weights[corresponding_index_in_embedding].data.numpy()\n",
    "    \n",
    "    except:\n",
    "        skipped_words=skipped_words+1\n",
    "        pass\n",
    "    \n",
    "    if corresponding_index_in_embedding is not None:\n",
    "        embedding_matrix_to_use[index]=numpy_corresponding\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference link- https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "embedding_layer=Embedding(len(word_index)+1,300,weights=[embedding_matrix_to_use],input_length=longest_sentence,trainable=False)\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "model_keras=Sequential()\n",
    "\n",
    "model_keras.add(embedding_layer)\n",
    "model_keras.add(LSTM(units=32,dropout=0.1))\n",
    "model_keras.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model_keras.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras.fit(X_train_with_padding,y_train,epochs=10,validation_data=(X_test_with_padding,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
