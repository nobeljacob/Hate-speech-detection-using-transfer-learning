{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 2 Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy import argmax\n",
    "import torch.nn as nn\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "data = pd.read_csv(r'bengali_hatespeech.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#: Splitting off a part of the Bengali corpus such that it roughly\n",
    "#equals the Hindi corpus in size and distribution of classes (hatespeech/non-hatespeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Includes positive and negative(non-hatespeech/hatespeech) data frame sets\n",
    "\n",
    "first_set_data= data.head(100)\n",
    "last_set_data=data.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the data frames into resultant single set of data frames with both positive and negative classes\n",
    "\n",
    "frames=[first_set_data, last_set_data]\n",
    "bengali_data = pd.concat(frames)\n",
    "bengali_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the data to lower text, remove punctuations if any and remove the emoji or emoticons\n",
    "\n",
    "bengali_data['sentence']= bengali_data['sentence'].str.lower()\n",
    "bengali_data['sentence'] = bengali_data['sentence'].str.replace('[!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]','')\n",
    "bengali_data['sentence'] = bengali_data['sentence'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying few data frames with columns of sentence as text and hate as class one(positive) and zero(negative)\n",
    "\n",
    "bengali_data.head()\n",
    "bengali_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of all Bengali words\n",
    "\n",
    "vocab = bengali_data['sentence'].values.tolist()\n",
    "vocab= [words for lines in vocab for words in lines.split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the probability to keep words in context\n",
    "\n",
    "bengali_freq = {} \n",
    "bengali_relative_freq={}\n",
    "p_keep_bengali={}\n",
    "for item in vocab: \n",
    "    if (item in bengali_freq): \n",
    "        bengali_freq[item] += 1\n",
    "    else: \n",
    "        bengali_freq[item] = 1\n",
    "\n",
    "\n",
    "for item in bengali_freq:\n",
    "    bengali_relative_freq[item]=bengali_freq[item]/len(vocab)\n",
    "    \n",
    "        \n",
    "        \n",
    "for x, y in bengali_relative_freq.items():\n",
    "    p_keep_bengali[x]=(np.sqrt(y/0.001) +1) * (0.001/y) \n",
    "\n",
    "\n",
    "\n",
    "print(p_keep_bengali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating one hot vector\n",
    "\n",
    "def word_to_one_hot(word):\n",
    "    one_hot_vector=np.zeros((len(bengali_freq)))\n",
    "    \n",
    "    index=list(bengali_freq.keys()).index(word)\n",
    "    one_hot_vector[index]=1\n",
    "    \n",
    "    return one_hot_vector\n",
    "       \n",
    "\n",
    "\n",
    "#---test data--#\n",
    "#corresponding_one_hot_encoded=word_to_one_hot('যততসব')\n",
    "\n",
    "#print(corresponding_one_hot_encoded)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = open(\"bengali_freq_dictionary.txt\", \"wb\")\n",
    "pickle.dump(bengali_freq, file)\n",
    "file.close()\n",
    "\n",
    "#bengali_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_prob(word):\n",
    "    return p_keep[word]\n",
    "\n",
    "#--test--#\n",
    "#print(sampling_prob('যততসব'))\n",
    "#----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "window_size = 5\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_context_helper(variable_sentence,current_index):\n",
    "    #takes in the new sentence and the curresponding index\n",
    "    \n",
    "    word_list=[]\n",
    "    i=current_index\n",
    "    j=current_index\n",
    "    lower_limit=current_index-window_size\n",
    "    upper_limit=current_index+window_size\n",
    "\n",
    "    if lower_limit<0:\n",
    "        lower_limit=0\n",
    "    if upper_limit> len(variable_sentence)-1:\n",
    "        upper_limit=len(variable_sentence)-1\n",
    "\n",
    "    while(i>lower_limit):\n",
    "        word_list.append(variable_sentence[i-1])\n",
    "        i=i-1\n",
    "\n",
    "    while (j<upper_limit):\n",
    "        word_list.append(variable_sentence[j+1])\n",
    "        j=j+1\n",
    "    \n",
    "    return variable_sentence[current_index], word_list\n",
    "    #returns the currespong word and the list of words in its context\n",
    "\n",
    "def get_target_context(sentence):\n",
    "    \n",
    "    sentence=sentence.split()\n",
    "    new_sentence=[]\n",
    "    for each_word in sentence:\n",
    "        float_value=float(p_keep_bengali[each_word])\n",
    "        if np.random.random() < float_value:\n",
    "            new_sentence.append(each_word)\n",
    "    \n",
    "    p=[]\n",
    "   \n",
    "    len_new=len(new_sentence)\n",
    "    for i in range (len_new):\n",
    "        #print(get_target_context_helper(new_sentence,i)) #passes the new sentences and each index to do the window\n",
    "        p.append(get_target_context_helper(new_sentence,i))\n",
    "\n",
    "        \n",
    "    return p\n",
    "\n",
    "\n",
    "each_sentence=bengali_data['sentence']\n",
    "list_of_all_words_and_corresponding_context_words=[]\n",
    "for every_sentence in each_sentence:\n",
    "    list_of_all_words_and_corresponding_context_words.append(get_target_context(every_sentence))\n",
    "    \n",
    "list_of_all_words_and_corresponding_context_words = [item for sublist in list_of_all_words_and_corresponding_context_words for item in sublist]\n",
    "print(list_of_all_words_and_corresponding_context_words)\n",
    "#returns the list of all the focus and context words for the entire vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More hyperparameters\n",
    "learning_rate = 0.025\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model \n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "  def __init__(self,embedding_size,Vocabulary_size):\n",
    "    super(Word2Vec,self).__init__()\n",
    "    \n",
    "    self.l1=nn.Linear(Vocabulary_size,embedding_size)\n",
    "    self.l2=nn.Linear(embedding_size,Vocabulary_size)\n",
    "    self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "  def forward(self, one_hot):\n",
    "    out=self.l1(one_hot)\n",
    "    out=self.l2(out)\n",
    "    out=self.softmax(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "model=Word2Vec(embedding_size,len(bengali_freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot to torch transform\n",
    "\n",
    "def transform_to_torch(corresponding_one_hot_encoded):\n",
    "    corresponding_one_hot_encoded_torch = torch.from_numpy(corresponding_one_hot_encoded.astype(np.float32))\n",
    "    corresponding_one_hot_encoded_torch = corresponding_one_hot_encoded_torch.view(corresponding_one_hot_encoded_torch.shape[0],1)\n",
    "    corresponding_one_hot_encoded_torch=corresponding_one_hot_encoded_torch.t()\n",
    "    return corresponding_one_hot_encoded_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train procedure\n",
    "\n",
    "# load initial weights\n",
    "\n",
    "n_iters=100\n",
    "\n",
    "def train():\n",
    "    for epoch in range(n_iters):\n",
    "        \n",
    "        for each_word in list_of_all_words_and_corresponding_context_words:\n",
    "            focus,context_words=each_word\n",
    "            #print(\"focus is\",focus)\n",
    "            focus =word_to_one_hot(focus)\n",
    "            focus =transform_to_torch(focus)\n",
    "            \n",
    "            \n",
    "            for each_context_word in context_words:\n",
    "                #print(\"contexts are\",each_context_word)\n",
    "                each_context_word=word_to_one_hot(each_context_word)\n",
    "                each_context_word =transform_to_torch(each_context_word)\n",
    "                Y_pred=model(focus)\n",
    "                #torch.max(labels, 1)[1]\n",
    "                loss=criterion(Y_pred,torch.max(each_context_word, 1)[1])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "        print(\"loss is\",loss)\n",
    "\n",
    "#train() #uncomment the train function to run for 100 epochs.\n",
    "\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model as bengali.pth\n",
    "\n",
    "model_path = F\"bengali.pth\"\n",
    "#torch.save(model.state_dict(), model_path)\n",
    "\n",
    "##for implementation representation the data was run on 200 rows. The code for the entire data set was run in collab and saved\n",
    "#bengali.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
